{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: This notebook is a continuation of my previous notebook which contained EDA.**   \n",
    "\n",
    "You can refer the following links:  \n",
    "- Part-1 EDA: LINK  \n",
    "- End-to-End implementation with deployment on AWS: https://github.com/Sharma-Ayush/Credit-Card-Fraud-Detection.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective for this notebook is:  \n",
    "- Feature Engineering.  \n",
    "- Handling Imbalance.  \n",
    "- Feature importance.  \n",
    "- Cross-validation and hyper parameter tuning of classification models.  \n",
    "- Testing on test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>PS:</u>  \n",
    "- Feel free to contact me if you have any doubts or feedback through the comment section or my socials.\n",
    "- Please upvote the notebook if you like it, as it would motivate me to develop more projects like these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Socials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow me on these platforms, for more such content:  \n",
    "\n",
    "LinkedIn: https://www.linkedin.com/in/ayush-sharma-660831125/  \n",
    "Github: https://github.com/Sharma-Ayush  \n",
    "Kaggle: https://www.kaggle.com/ayushsharma0812"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Mainpulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Machine Learning\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Extra modules\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom classes & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_log_transform(x):\n",
    "    '''Transform x by taking the log of the data after shifting by 1. This operation is done two times iteratively.'''\n",
    "    return np.log10(np.log10(x + 1) + 1)\n",
    "\n",
    "def cube_root_transform(x):\n",
    "    '''Transform x by taking the cube root of the data.'''\n",
    "    return np.cbrt(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Data/creditcard_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already seen the huge imbalance that exists within our dataset. We need to mitigate that imbalance, otherwise our model will focus more on the majority class(genuine transactions). There are many techniques that we can use:  \n",
    "\n",
    "1. <u>Undersampling of majority class:</u> We take random sample of majority class so that the size matches that of minority class. This will reduce the dataset size which is good if we want reduced computation effort and thus, will lead to faster computation but at the same time it will lead to loss of information.  \n",
    "\n",
    "2. <u>Oversampling of minority class:</u> We do random sampling on minority class with replacement so that the size of minority class matches that of majority class. This will increase dataset's size which will lead to more computational effort but will preserve information unlike undersampling. Duplicates are generated within minority class and this can lead to overfitting.  \n",
    "\n",
    "3. <u>Synthetic data generation techniques:</u> We use techniques like SMOTE, its variants or ADASYN to oversample minority class in a systematic way by adding points in between existing minority class data points with some logic behind. This is the best way to upsample as duplication is not present but points are added along straight lines in between existing data points and this can lead to addition of artificial patterns. \n",
    "\n",
    "4. <u>Weight based sensitivity:</u> Many algorithms allow us to specify weights for each record of our dataset, we can specify higher weights to minority class records and the algorithm will focus more on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to try both synthetic generation and weight based sensitivity, then compare them but the dataset is decently big in size and I want to perform cross-validaiton for hyper parameter tuning as well and thus, there is a problem here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we generate cross-validation sets they should be first split up and then each of the set should be passed through the synthetic generator, any other transformations and then finally evaluated upon. This is done after splitting so that the information doesn't leak in between train and validation sets. To save time on computation and still do hyperparameter tuning which itself will be time taking, I want a technique of synthetic generation where the size of the resulting dataset is not too big because I don't have that much time and computation to spare on both - mulltiple synthetic generations and cross-validation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what should I do? Definitely, I can try weight based sensitivity as it is, because it does not involve any synthetic generation. Other option that I am thinking of is a combination of undersampling and ADASYN. If I was to undersample majority class all the way to same size as minority class then that will be too much loss of information. Due to my own time and computation constraints, I want to just reduce the size of majority class by enough and at the same time oversample minority class through synthetic generator such that the time taken for this process is managable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use ADASYN. ADSYN tries to add more data towards the low density points of minority class. This means that even noisy points of fraudulent class will get attention and it might overfit a bit too much on the minority class i.e. it might perform very well on minority class but poorer on majority class. In a case like this of frauds and scams we want to primarily focus on predicting fraud cases as best as possible and even if some genuine cases(as long as its not too much above some threshold) are predicted to be as fraud we should be okay with it as the credi card issuer can provide tighter verification to user in these cases. This way we have tight leash on frauds which will save the company money and tighter verification for genuine cases predicted as fraud. If its too much overfitting we can try other techniques like SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersampling will be done before training. I will choose a decent size of 10k for undersampling and then upsample minority class to this using ADASYN. Anyone who can spare time and computational effort on this can use the whole dataset as it is and upsample minority class, this will preserve information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "minority_class_size = df_train.Class.value_counts().min()\n",
    "\n",
    "# Random under sampling the majority class to a size of 10000\n",
    "X_resampled, Y_resampled = RandomUnderSampler(sampling_strategy = minority_class_size/10000, random_state = 42).fit_resample(df_train.drop(columns = 'Class'), df_train['Class'])\n",
    "\n",
    "# ADASYN resampler fo synthetic generation of minority class\n",
    "ADASYN_resampler = ADASYN(sampling_strategy = 'auto', random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the EDA notebook, I have to perform following transformations and manipulations on my dataset:  \n",
    "- Take log10 Amount column after shiting the data by 1. This is done two times iteratively.  \n",
    "- Drop 3 columns: V13, V15, and V23.  \n",
    "- Take cube root of all the rest of PCA encoded columns.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of the transformations will be as followed:  \n",
    "- Transform columns based on EDA.  \n",
    "- Standardize columns so that scales of columns are similar.  \n",
    "- Then in the last use ADASYN for upsampling minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to be transformed\n",
    "double_log_transform_columns = ['Amount']\n",
    "cube_root_transform_columns = ['V' + str(i) for i in range(1, 29) if i not in [13, 15, 23]]\n",
    "passthrough_columns = ['Time']\n",
    "\n",
    "# Define the functional transformers for the group of columns\n",
    "double_log_transformer = FunctionTransformer(func = double_log_transform, feature_names_out = 'one-to-one')\n",
    "cube_root_transformer = FunctionTransformer(func = cube_root_transform, feature_names_out = 'one-to-one')\n",
    "pass_through_transformer = 'passthrough'\n",
    "\n",
    "# Create transformers\n",
    "column_transformer = ColumnTransformer([('double_log_transform', double_log_transformer, double_log_transform_columns),\n",
    "                                        ('cube_root_transformer', cube_root_transformer, cube_root_transform_columns),\n",
    "                                        ('passthrough', pass_through_transformer, passthrough_columns)],\n",
    "                                        remainder = 'drop')\n",
    "standard_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.kdnuggets.com/2023/01/7-smote-variations-oversampling.html\n",
    "- https://www.analyticsvidhya.com/blog/2020/10/overcoming-class-imbalance-using-smote-techniques/#h-dealing-with-imbalanced-data\n",
    "- https://www.youtube.com/@krishnaik06  \n",
    "- Hands on Machine Learning with Scikit-Learn & TensorFlow by Aurélien Géron (O'Reilly). CopyRight 2017 Aurélien Géron"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
