{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: This notebook is a continuation of my previous notebook which contained EDA.**   \n",
    "\n",
    "You can refer the following links:  \n",
    "- Part-1 EDA: LINK  \n",
    "- End-to-End implementation with deployment on AWS: https://github.com/Sharma-Ayush/Credit-Card-Fraud-Detection.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective for this notebook is:  \n",
    "- Feature Engineering.  \n",
    "- Handling Imbalance.  \n",
    "- Feature importance.  \n",
    "- Cross-validation and hyper parameter tuning of classification models.  \n",
    "- Testing on test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>PS:</u>  \n",
    "- Feel free to contact me if you have any doubts or feedback through the comment section or my socials.\n",
    "- Please upvote the notebook if you like it, as it would motivate me to develop more projects like these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Socials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow me on these platforms, for more such content:  \n",
    "\n",
    "LinkedIn: https://www.linkedin.com/in/ayush-sharma-660831125/  \n",
    "Github: https://github.com/Sharma-Ayush  \n",
    "Kaggle: https://www.kaggle.com/ayushsharma0812"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import ADASYN, SMOTE\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import tensorflow as tf\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "from sklearn.metrics import auc, precision_recall_curve\n",
    "from sklearn.metrics._scorer import make_scorer\n",
    "\n",
    "# extra modules\n",
    "from pathlib import Path\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom classes & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_log_transform(x):\n",
    "    '''Transform x by taking the log of the data after shifting by 1. This operation is done two times iteratively.'''\n",
    "    return np.log10(np.log10(x + 1) + 1)\n",
    "\n",
    "def cube_root_transform(x):\n",
    "    '''Transform x by taking the cube root of the data.'''\n",
    "    return np.cbrt(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_under_precision_recall_curve(y_true, y_pred):\n",
    "    '''Function that can compute the area under precision recall curve'''\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    auc_precision_recall = auc(recall, precision)\n",
    "    return auc_precision_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_cv_params(X, Y, training_pipeline, params, scoring, score_name, n_iter = 25, search_type = 'random', cv = 5):\n",
    "    '''Perform random search or grid search on given X and Y using given model & parameter distributions preceeded by column_trasnformer, standard_scaler & resampler.\n",
    "    n_iter(for random search), cv(no. of splits), scoring(score for finding best params) can be passed as parameters.'''\n",
    "    \n",
    "    if search_type == 'random':\n",
    "        cross_validator = RandomizedSearchCV(training_pipeline,\n",
    "                                             param_distributions = params,\n",
    "                                             n_iter = n_iter,\n",
    "                                             cv = cv,\n",
    "                                             scoring = scoring,\n",
    "                                             return_train_score = True,\n",
    "                                             refit = False,\n",
    "                                             error_score = 0,\n",
    "                                             n_jobs = -1,\n",
    "                                             random_state = 42)\n",
    "    elif search_type == 'grid':\n",
    "        cross_validator = GridSearchCV(training_pipeline,\n",
    "                                       param_grid = params,\n",
    "                                       scoring = scoring,\n",
    "                                       cv = cv,\n",
    "                                       n_jobs = -1,\n",
    "                                       refit = False,\n",
    "                                       error_score = 0,\n",
    "                                       return_train_score = True)\n",
    "    else:\n",
    "        print('Invalid search type. Aborting.')\n",
    "        return None\n",
    "\n",
    "    cross_validator.fit(X, Y)\n",
    "    print(f'For model - {training_pipeline.steps[-1][0]} {search_type} search:')\n",
    "    print('----------------------------------------------------------')\n",
    "    print(f'Best mean {score_name}:')\n",
    "    print(f'Train set: {np.round(max(cross_validator.cv_results_['mean_train_score']), 3)}')\n",
    "    print(f'Validation set: {np.round(cross_validator.best_score_, 3)}')\n",
    "    print('----------------------------------------------------------')\n",
    "\n",
    "    if search_type == 'random':\n",
    "        return cross_validator.best_params_\n",
    "    elif search_type == 'grid':\n",
    "        print(cross_validator.best_params_)\n",
    "        return cross_validator.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_boxplot_for_multi_num_cols(data, num_cols, target_col, color_mapping = None):\n",
    "    '''Plot boxplot of numerical columns segregated per category of target_col with given color mapping for multiple numerical columns.'''\n",
    "    \n",
    "    # Calculate number of figures needed for 5 columns per figure or lesser\n",
    "    no_of_figures = len(num_cols) // 5\n",
    "\n",
    "    if len(num_cols) % 5 != 0:\n",
    "        no_of_figures += 1\n",
    "\n",
    "    # Plotting the histograms and density plots\n",
    "    for i in range(no_of_figures):\n",
    "        columns_to_plot = num_cols[i*5:(i+1)*5]\n",
    "\n",
    "        fig, ax = plt.subplots(1, len(columns_to_plot), figsize = (len(columns_to_plot)*6, 4), sharex = True)\n",
    "\n",
    "        for index, col in enumerate(columns_to_plot):\n",
    "\n",
    "            if color_mapping != None:\n",
    "                sns.boxplot(data, y = col, hue = target_col, ax = ax[index], palette = color_mapping)\n",
    "            else:\n",
    "                sns.boxplot(data, y = col, hue = target_col, ax = ax[index])\n",
    "            ax[index].set_title(col)\n",
    "\n",
    "        plt.subplots_adjust(wspace = 0.25)\n",
    "\n",
    "        plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_logdir(neurons, layers, lr, batch_size, root_logdir = 'my_logs'):\n",
    "    '''Return path for log directory. This will help in generating new path for each model run.'''\n",
    "    return f'{Path(root_logdir)}\\\\Run_{neurons}_{layers}_{lr}_{batch_size}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Data/creditcard_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already seen the huge imbalance that exists within our dataset. We need to mitigate that imbalance, otherwise our model will focus more on the majority class(genuine transactions). There are many techniques that we can use:  \n",
    "\n",
    "1. <u>Undersampling of majority class:</u> We take random sample of majority class so that the size matches that of minority class. This will reduce the dataset size which is good if we want reduced computation effort and thus, will lead to faster computation but at the same time it will lead to loss of information.  \n",
    "\n",
    "2. <u>Oversampling of minority class:</u> We do random sampling on minority class with replacement so that the size of minority class matches that of majority class. This will increase dataset's size which will lead to more computational effort but will preserve information unlike undersampling. Duplicates are generated within minority class and this can lead to overfitting.  \n",
    "\n",
    "3. <u>Synthetic data generation techniques:</u> We use techniques like SMOTE, its variants or ADASYN to oversample minority class in a systematic way by adding points in between existing minority class data points with some logic behind. This is the best way to upsample as duplication is not present but points are added along straight lines in between existing data points and this can lead to addition of artificial patterns. \n",
    "\n",
    "4. <u>Weight based sensitivity:</u> Many algorithms allow us to specify weights for each record of our dataset, we can specify higher weights to minority class records and the algorithm will focus more on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to try both synthetic generation and weight based sensitivity, then compare them but the dataset is decently big in size and I want to perform cross-validaiton for hyper parameter tuning as well and thus, there is a problem here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we generate cross-validation sets they should be first split up and then each of the set should be passed through the synthetic generator, any other transformations and then finally evaluated upon. This is done after splitting so that the information doesn't leak in between train and validation sets. To save time on computation and still do hyperparameter tuning which itself will be time taking, I want a technique of synthetic generation where the size of the resulting dataset is not too big because I don't have that much time and computation to spare on both - mulltiple synthetic generations and cross-validation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, what should I do? Definitely, I can try weight based sensitivity as it is, because it does not involve any synthetic generation. Other option that I am thinking of is a combination of undersampling and ADASYN. If I was to undersample majority class all the way to same size as minority class then that will be too much loss of information. Due to my own time and computation constraints, I want to just reduce the size of majority class by enough and at the same time oversample minority class through synthetic generator such that the time taken for this process is managable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use ADASYN. ADSYN tries to add more data towards the low density points of minority class. This means that even noisy points of fraudulent class will get attention and it might overfit a bit too much on the minority class i.e. it might perform very well on minority class but poorer on majority class. In a case like this of frauds and scams we want to primarily focus on predicting fraud cases as best as possible and even if some genuine cases(as long as its not too much above some threshold) are predicted to be as fraud we should be okay with it as the credi card issuer can provide tighter verification to user in these cases. This way we have tight leash on frauds which will save the company money and tighter verification for genuine cases predicted as fraud. If its too much overfitting we can try other techniques like SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Undersampling will be done before training. I will choose a decent size of 10k for undersampling and then upsample minority class to this using ADASYN. Anyone who can spare time and computational effort on this can use the whole dataset as it is and upsample minority class, this will preserve information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "minority_class_size = df_train.Class.value_counts().min()\n",
    "\n",
    "# Random under sampling the majority class to a size of 10000\n",
    "X_resampled, Y_resampled = RandomUnderSampler(sampling_strategy = minority_class_size/10000, random_state = 42).fit_resample(df_train.drop(columns = 'Class'), df_train['Class'])\n",
    "\n",
    "# ADASYN resampler fo synthetic generation of minority class\n",
    "ADASYN_resampler = ADASYN(sampling_strategy = 'auto', random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the EDA notebook, I have to perform following transformations and manipulations on my dataset:  \n",
    "- Take log10 Amount column after shiting the data by 1. This is done two times iteratively.  \n",
    "- Drop 3 columns: V13, V15, and V23.  \n",
    "- Take cube root of all the rest of PCA encoded columns.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order of the transformations will be as followed:  \n",
    "- Transform columns based on EDA.  \n",
    "- Standardize columns so that scales of columns are similar.  \n",
    "- Then in the last use ADASYN for upsampling minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to be transformed\n",
    "double_log_transform_columns = ['Amount']\n",
    "cube_root_transform_columns = ['V' + str(i) for i in range(1, 29) if i not in [13, 15, 23]]\n",
    "passthrough_columns = ['Time']\n",
    "drop_columns = ['V13', 'V15', 'V23']\n",
    "\n",
    "# Define the functional transformers for the group of columns\n",
    "double_log_transformer = FunctionTransformer(func = double_log_transform, feature_names_out = 'one-to-one')\n",
    "cube_root_transformer = FunctionTransformer(func = cube_root_transform, feature_names_out = 'one-to-one')\n",
    "pass_through_transformer = 'passthrough'\n",
    "\n",
    "# Create transformers\n",
    "column_transformer = ColumnTransformer([('double_log_transform', double_log_transformer, double_log_transform_columns),\n",
    "                                        ('cube_root_transformer', cube_root_transformer, cube_root_transform_columns),\n",
    "                                        ('passthrough', pass_through_transformer, passthrough_columns)],\n",
    "                                        remainder = 'drop')\n",
    "standard_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What metric to choose for evaluation?**  \n",
    "Since its an imbalanced dataset, we can't use accuracy as an evaluation metric as that would be highly influenced by the majority class.  \n",
    "\n",
    "If we were to work on this problem statement in a business setting, then we would have some criteria on precision and recall that the final model should achieve to be actually useful and deployed into production. Since we want to focus more on fraud to keep chargeback costs to lower amount, we would have had stricter criteria on recall so that the prediction within fraud class is quite good and a more relaxed criteria on precision meaning that we can allow more errors in genuine cases. But these are two metrics, how can we use two metrics to properly compare several models? That's why measures which combine these precision and recall into one measure are used for model comparison. Once the final model in decided based on this summarization measure, we can adjust the threshold on the soft probability to adjust precision and recall levels of the model. If then we are able to achieve precision and recall above our business criterias, we can go forward to deploying our model otherwise we go back to the drawing board.  \n",
    "\n",
    "For evaluating, imbalanced datasets F1 score, area under the ROC and area under the precision-recall curve are common measures. For very high imbalanced dataset, precision-recall curve area is seen to be more sensitive and thus, better towards summarizing the performance. Therefore, F1 score or area under the precision-recall curve can be used here. As per the datasets author they have suggested to use area under precision-recall curve and that's what I will use here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the evaluation metric scorer object\n",
    "area_under_precision_recall_curve_scorer = make_scorer(area_under_precision_recall_curve, response_method = 'predict_proba', greater_is_better = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimenting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For experimental purpose, I will just use logistic regression model.  \n",
    "\n",
    "I will try to see the following things:  \n",
    "1. Class weight balancing vs ADASYN resampler  \n",
    "2. Does column transformation have any effect on performance or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Class weight balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - Logistic_Regression random search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.927\n",
      "Validation set: 0.914\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Logistic_Regression__solver': 'sag',\n",
       " 'Logistic_Regression__penalty': 'l2',\n",
       " 'Logistic_Regression__l1_ratio': np.float64(0.8),\n",
       " 'Logistic_Regression__C': np.float64(0.012742749857031334)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing random search cross-validation\n",
    "\n",
    "find_best_cv_params(X_resampled,\n",
    "                    Y_resampled,\n",
    "                    Pipeline([('column_transformer', column_transformer), \n",
    "                              ('standard_scaler', standard_scaler),\n",
    "                              ('Logistic_Regression', LogisticRegression(class_weight = 'balanced', n_jobs = -1, random_state = 123))]), \n",
    "                    {'Logistic_Regression__penalty' : ['l1', 'l2', 'elasticnet', None],\n",
    "                     'Logistic_Regression__C' : np.logspace(-4, 4, 20), \n",
    "                     'Logistic_Regression__solver': ['lbfgs', 'newton-cg', 'sag', 'saga'],\n",
    "                     'Logistic_Regression__l1_ratio': np.linspace(0.1, 1, 9, endpoint = False)}, \n",
    "                    area_under_precision_recall_curve_scorer,\n",
    "                    'area under precision recall curve',\n",
    "                    n_iter = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - Logistic_Regression grid search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.926\n",
      "Validation set: 0.916\n",
      "----------------------------------------------------------\n",
      "{'Logistic_Regression__C': np.float64(0.021544346900318832), 'Logistic_Regression__l1_ratio': np.float64(0.55), 'Logistic_Regression__penalty': 'elasticnet', 'Logistic_Regression__solver': 'saga'}\n"
     ]
    }
   ],
   "source": [
    "# Performing grid search cross-validation\n",
    "\n",
    "best_params = find_best_cv_params(X_resampled,\n",
    "                                  Y_resampled,\n",
    "                    Pipeline([('column_transformer', column_transformer), \n",
    "                              ('standard_scaler', standard_scaler),\n",
    "                              ('Logistic_Regression', LogisticRegression(class_weight = 'balanced', n_jobs = -1, random_state = 123))]),\n",
    "                                  {'Logistic_Regression__penalty' : ['elasticnet'],\n",
    "                                  'Logistic_Regression__C' : np.logspace(-3, -1, 10), \n",
    "                                  'Logistic_Regression__solver': ['saga'],\n",
    "                                  'Logistic_Regression__l1_ratio': np.linspace(0.5, 0.95, 10, endpoint = True)}, \n",
    "                                  area_under_precision_recall_curve_scorer,\n",
    "                                  'area under precision recall curve',\n",
    "                                  search_type = 'grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. ADASYN resampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - Logistic_Regression random search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.753\n",
      "Validation set: 0.728\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Logistic_Regression__solver': 'newton-cg',\n",
       " 'Logistic_Regression__penalty': None,\n",
       " 'Logistic_Regression__l1_ratio': np.float64(0.1),\n",
       " 'Logistic_Regression__C': np.float64(1.623776739188721)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing random search cross-validation\n",
    "\n",
    "find_best_cv_params(X_resampled,\n",
    "                    Y_resampled,\n",
    "                    Pipeline([('column_transformer', column_transformer), \n",
    "                              ('standard_scaler', standard_scaler),\n",
    "                              ('ADASYN_resample', ADASYN_resampler),\n",
    "                              ('Logistic_Regression', LogisticRegression(n_jobs = -1, random_state = 123))]),\n",
    "                    {'Logistic_Regression__penalty' : ['l1', 'l2', 'elasticnet', None],\n",
    "                     'Logistic_Regression__C' : np.logspace(-4, 4, 20), \n",
    "                     'Logistic_Regression__solver': ['lbfgs', 'newton-cg', 'sag', 'saga'],\n",
    "                     'Logistic_Regression__l1_ratio': np.linspace(0.1, 1, 9, endpoint = False)}, \n",
    "                    area_under_precision_recall_curve_scorer,\n",
    "                    'area under precision recall curve',\n",
    "                    n_iter = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like its undefitting, even though the logistic regression model has no regularization i.e. the complexity of model is at its maximum it can be. Does it mean that using ADASYN has introduced artificial noise thus making the patterns complexer? Because if data has become more complex and model is simpler that would lead to underfitting. Maybe we can also play with parameters of ADASYN and hope things change but I won't try this here. Rather I want to try SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. SMOTE resampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - Logistic_Regression random search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.925\n",
      "Validation set: 0.912\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Logistic_Regression__solver': 'sag',\n",
       " 'Logistic_Regression__penalty': 'l2',\n",
       " 'Logistic_Regression__l1_ratio': np.float64(0.8),\n",
       " 'Logistic_Regression__C': np.float64(0.012742749857031334)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SMOTE_resampler = SMOTE(random_state = 42)\n",
    "\n",
    "# Performing random search cross-validation\n",
    "\n",
    "find_best_cv_params(X_resampled,\n",
    "                    Y_resampled,\n",
    "                    Pipeline([('column_transformer', column_transformer), \n",
    "                              ('standard_scaler', standard_scaler),\n",
    "                              ('SMOTE_resample', SMOTE_resampler),\n",
    "                              ('Logistic_Regression', LogisticRegression(n_jobs = -1, random_state = 123))]),\n",
    "                    {'Logistic_Regression__penalty' : ['l1', 'l2', 'elasticnet', None],\n",
    "                     'Logistic_Regression__C' : np.logspace(-4, 4, 20), \n",
    "                     'Logistic_Regression__solver': ['lbfgs', 'newton-cg', 'sag', 'saga'],\n",
    "                     'Logistic_Regression__l1_ratio': np.linspace(0.1, 1, 9, endpoint = False)}, \n",
    "                    area_under_precision_recall_curve_scorer,\n",
    "                    'area under precision recall curve',\n",
    "                    n_iter = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE tends to work much better than ADASYN in this problem statement. ADASYN tries to add artificial points more and more near  the exisitng points which are farther from the center of minority class i.e. points which are mostly surrounded by majority class points. But what if these points are pure noisy points which we don't want to focus on then ADASYN will force us to focus on those points unnecessarily and this will affect our performance. SMOTE on the other hand adds new points randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - Logistic_Regression grid search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.925\n",
      "Validation set: 0.915\n",
      "----------------------------------------------------------\n",
      "{'Logistic_Regression__C': np.float64(0.007742636826811269), 'Logistic_Regression__l1_ratio': np.float64(0.6), 'Logistic_Regression__penalty': 'elasticnet', 'Logistic_Regression__solver': 'saga'}\n"
     ]
    }
   ],
   "source": [
    "# Performing grid search cross-validation\n",
    "\n",
    "best_params = find_best_cv_params(X_resampled,\n",
    "                                  Y_resampled,\n",
    "                                  Pipeline([('column_transformer', column_transformer), \n",
    "                                              ('standard_scaler', standard_scaler),\n",
    "                                              ('SMOTE_resample', SMOTE_resampler),\n",
    "                                              ('Logistic_Regression', LogisticRegression(n_jobs = -1, random_state = 123))]),\n",
    "                                  {'Logistic_Regression__penalty' : ['elasticnet'],\n",
    "                                  'Logistic_Regression__C' : np.logspace(-3, -1, 10), \n",
    "                                  'Logistic_Regression__solver': ['saga'],\n",
    "                                  'Logistic_Regression__l1_ratio': np.linspace(0.5, 0.95, 10, endpoint = True)}, \n",
    "                                  area_under_precision_recall_curve_scorer,\n",
    "                                  'area under precision recall curve',\n",
    "                                  search_type = 'grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class weight balancing performs slighlty better than SMOTE. Going on ahead we will work with class weight balancing only, this will reduce the need for resampling in our training pipeline. I can also increase my resampling size for the majority class and that will help in adding more information but I will stick with this for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Class weight balancing without Amount column transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - Logistic_Regression random search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.923\n",
      "Validation set: 0.914\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Logistic_Regression__solver': 'sag',\n",
       " 'Logistic_Regression__penalty': 'l2',\n",
       " 'Logistic_Regression__l1_ratio': np.float64(0.6),\n",
       " 'Logistic_Regression__C': np.float64(0.08858667904100823)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_transformer_1 = ColumnTransformer([('cube_root_transform', cube_root_transformer, cube_root_transform_columns),\n",
    "                                          ('drop', 'drop', drop_columns)],\n",
    "                                          remainder = 'passthrough')\n",
    "\n",
    "# Performing random search cross-validation\n",
    "\n",
    "find_best_cv_params(X_resampled,\n",
    "                    Y_resampled,\n",
    "                    Pipeline([('column_transformer', column_transformer_1), \n",
    "                              ('standard_scaler', standard_scaler),\n",
    "                              ('Logistic_Regression', LogisticRegression(class_weight = 'balanced', n_jobs = -1, random_state = 123))]),\n",
    "                    {'Logistic_Regression__penalty' : ['l1', 'l2', 'elasticnet', None],\n",
    "                     'Logistic_Regression__C' : np.logspace(-4, 4, 20), \n",
    "                     'Logistic_Regression__solver': ['lbfgs', 'newton-cg', 'sag', 'saga'],\n",
    "                     'Logistic_Regression__l1_ratio': np.linspace(0.1, 1, 9, endpoint = False)}, \n",
    "                    area_under_precision_recall_curve_scorer,\n",
    "                    'area under precision recall curve',\n",
    "                    n_iter = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - Logistic_Regression grid search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.923\n",
      "Validation set: 0.914\n",
      "----------------------------------------------------------\n",
      "{'Logistic_Regression__C': np.float64(0.026366508987303583), 'Logistic_Regression__penalty': 'l2', 'Logistic_Regression__solver': 'sag'}\n"
     ]
    }
   ],
   "source": [
    "# Performing grid search cross-validation\n",
    "\n",
    "best_params = find_best_cv_params(X_resampled,\n",
    "                                  Y_resampled,\n",
    "                                  Pipeline([('column_transformer', column_transformer_1), \n",
    "                                              ('standard_scaler', standard_scaler),\n",
    "                                              ('Logistic_Regression', LogisticRegression(class_weight = 'balanced', n_jobs = -1, random_state = 123))]),\n",
    "                                  {'Logistic_Regression__penalty' : ['l2'],\n",
    "                                  'Logistic_Regression__C' : np.logspace(-2, 0, 20), \n",
    "                                  'Logistic_Regression__solver': ['sag']}, \n",
    "                                  area_under_precision_recall_curve_scorer,\n",
    "                                  'area under precision recall curve',\n",
    "                                  search_type = 'grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no 0.2% drop in performance. Thus, the transformation on amount is benefecial even though by very small amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Class weight balancing without V1 to V28 column transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - Logistic_Regression random search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.936\n",
      "Validation set: 0.924\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Logistic_Regression__solver': 'lbfgs',\n",
       " 'Logistic_Regression__penalty': None,\n",
       " 'Logistic_Regression__l1_ratio': np.float64(0.6),\n",
       " 'Logistic_Regression__C': np.float64(206.913808111479)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_transformer_2 = ColumnTransformer([('double_log_transform', double_log_transformer, double_log_transform_columns),\n",
    "                                          ('drop', 'drop', drop_columns)],\n",
    "                                          remainder = 'passthrough')\n",
    "\n",
    "# Performing random search cross-validation\n",
    "\n",
    "find_best_cv_params(X_resampled,\n",
    "                    Y_resampled,\n",
    "                    Pipeline([('column_transformer', column_transformer_2), \n",
    "                              ('standard_scaler', standard_scaler),\n",
    "                              ('Logistic_Regression', LogisticRegression(class_weight = 'balanced', n_jobs = -1, random_state = 123))]),\n",
    "                    {'Logistic_Regression__penalty' : ['l1', 'l2', 'elasticnet', None],\n",
    "                     'Logistic_Regression__C' : np.logspace(-4, 4, 20), \n",
    "                     'Logistic_Regression__solver': ['lbfgs', 'newton-cg', 'sag', 'saga'],\n",
    "                     'Logistic_Regression__l1_ratio': np.linspace(0.1, 1, 9, endpoint = False)}, \n",
    "                    area_under_precision_recall_curve_scorer,\n",
    "                    'area under precision recall curve',\n",
    "                    n_iter = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - Logistic_Regression grid search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.936\n",
      "Validation set: 0.924\n",
      "----------------------------------------------------------\n",
      "{'Logistic_Regression__penalty': None, 'Logistic_Regression__solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "# Performing grid search cross-validation\n",
    "\n",
    "best_params = find_best_cv_params(X_resampled,\n",
    "                                  Y_resampled,\n",
    "                                  Pipeline([('column_transformer', column_transformer_2), \n",
    "                                              ('standard_scaler', standard_scaler),\n",
    "                                              ('Logistic_Regression', LogisticRegression(class_weight = 'balanced', n_jobs = -1, random_state = 123))]),\n",
    "                                  {'Logistic_Regression__penalty' : [None],\n",
    "                                  'Logistic_Regression__solver': ['lbfgs']}, \n",
    "                                  area_under_precision_recall_curve_scorer,\n",
    "                                  'area under precision recall curve',\n",
    "                                  search_type = 'grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performance has improved compared to when the transformations were performed. The transformation was done from the persepective that even extreme points should have similar weightage when it comes to training so that the algorithm can perform better throughout the range of these predictor features but this increase in performance implies that these extreme points on which when we focussed more lead to poorer performance overall, this means that these points could actually be outliers which don't follow similar pattern as majority of data points. Also, our model is quite simple and linear in nature. Thus, if the patterns become non-linear near these rextreme points then our model would not be able to model it appropriately, this means that we might need more complex models. Thus, I should try more complex model like decision tree first before we jump to more conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Decision tree with class weight balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - Decision_Tree random search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.891\n",
      "Validation set: 0.879\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Decision_Tree__splitter': 'best',\n",
       " 'Decision_Tree__min_samples_leaf': 10,\n",
       " 'Decision_Tree__max_features': None,\n",
       " 'Decision_Tree__max_depth': 2,\n",
       " 'Decision_Tree__criterion': 'log_loss'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing random search cross-validation\n",
    "\n",
    "find_best_cv_params(X_resampled,\n",
    "                    Y_resampled,\n",
    "                    Pipeline([('column_transformer', column_transformer), \n",
    "                              ('standard_scaler', standard_scaler),\n",
    "                              ('Decision_Tree', DecisionTreeClassifier(class_weight = 'balanced', random_state = 123))]),                    \n",
    "                    {'Decision_Tree__max_features': [None, 'sqrt', 'log2'],\n",
    "                     'Decision_Tree__min_samples_leaf': list(range(5, 51, 5)),\n",
    "                     'Decision_Tree__max_depth' : list(range(1, 3, 1)),\n",
    "                     'Decision_Tree__splitter': ['best', 'random'],\n",
    "                     'Decision_Tree__criterion' :['gini', 'entropy', 'log_loss']}, \n",
    "                    area_under_precision_recall_curve_scorer,\n",
    "                    'area under precision recall curve',\n",
    "                    n_iter = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - Decision_Tree grid search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.891\n",
      "Validation set: 0.879\n",
      "----------------------------------------------------------\n",
      "{'Decision_Tree__criterion': 'entropy', 'Decision_Tree__max_depth': 2, 'Decision_Tree__max_features': None, 'Decision_Tree__min_samples_leaf': 1, 'Decision_Tree__splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "# Performing random search cross-validation\n",
    "\n",
    "best_params = find_best_cv_params(X_resampled,\n",
    "                                  Y_resampled,\n",
    "                                  Pipeline([('column_transformer', column_transformer), \n",
    "                                              ('standard_scaler', standard_scaler),\n",
    "                                              ('Decision_Tree', DecisionTreeClassifier(class_weight = 'balanced', random_state = 123))]),                    \n",
    "                                  {'Decision_Tree__max_features': [None, 'sqrt', 'log2'],\n",
    "                                   'Decision_Tree__min_samples_leaf': list(range(1, 21, 2)),\n",
    "                                   'Decision_Tree__max_depth' : list(range(2, 3, 1)),\n",
    "                                   'Decision_Tree__splitter': ['best', 'random'],\n",
    "                                   'Decision_Tree__criterion' :['gini', 'entropy', 'log_loss']}, \n",
    "                                   area_under_precision_recall_curve_scorer,\n",
    "                                  'area under precision recall curve',\n",
    "                                   search_type = 'grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, there is not too much complexity in the dataset decision boundaries based on following two observations:  \n",
    "1. Logistic regression performed better than decision tree model.  \n",
    "2. The depth of the tree is not too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there is one problem with decision trees and that is it does piecewise approximation of the decision boundaries which can be detrimental for performance in case the decision boundaries are not parallel to the predictor axes and thus, it might even perfom poorer than logistic regression in this scenario. One thing that can be tried here is PCA which maybe help in the case that the decision boundaries are linear but not for when non-linearity comes in non-linear situations. Lets try PCA as well. But most of the columns went through PCA already, only two columns Amount and Time are new. Thus, it might not have much effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. Decision tree with class weight balancing and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - Decision_Tree random search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.893\n",
      "Validation set: 0.886\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Decision_Tree__splitter': 'best',\n",
       " 'Decision_Tree__min_samples_leaf': 10,\n",
       " 'Decision_Tree__max_features': None,\n",
       " 'Decision_Tree__max_depth': 2,\n",
       " 'Decision_Tree__criterion': 'log_loss'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing random search cross-validation\n",
    "\n",
    "find_best_cv_params(X_resampled,\n",
    "                    Y_resampled,\n",
    "                    Pipeline([('column_transformer', column_transformer), \n",
    "                              ('pca', PCA(random_state = 42)),\n",
    "                              ('standard_scaler', standard_scaler),\n",
    "                              ('Decision_Tree', DecisionTreeClassifier(class_weight = 'balanced', random_state = 123))]),                    \n",
    "                    {'Decision_Tree__max_features': [None, 'sqrt', 'log2'],\n",
    "                     'Decision_Tree__min_samples_leaf': list(range(5, 51, 5)),\n",
    "                     'Decision_Tree__max_depth' : list(range(1, 3, 1)),\n",
    "                     'Decision_Tree__splitter': ['best', 'random'],\n",
    "                     'Decision_Tree__criterion' :['gini', 'entropy', 'log_loss']}, \n",
    "                     area_under_precision_recall_curve_scorer,\n",
    "                     'area under precision recall curve',\n",
    "                     n_iter = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - Decision_Tree grid search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.893\n",
      "Validation set: 0.886\n",
      "----------------------------------------------------------\n",
      "{'Decision_Tree__criterion': 'entropy', 'Decision_Tree__max_depth': 2, 'Decision_Tree__max_features': None, 'Decision_Tree__min_samples_leaf': 1, 'Decision_Tree__splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "# Performing random search cross-validation\n",
    "\n",
    "best_params = find_best_cv_params(X_resampled,\n",
    "                                  Y_resampled,\n",
    "                                  Pipeline([('column_transformer', column_transformer), \n",
    "                                            ('pca', PCA(random_state = 42)),\n",
    "                                            ('standard_scaler', standard_scaler),\n",
    "                                            ('Decision_Tree', DecisionTreeClassifier(class_weight = 'balanced', random_state = 123))]),                    \n",
    "                                  {'Decision_Tree__max_features': [None, 'sqrt', 'log2'],\n",
    "                                   'Decision_Tree__min_samples_leaf': list(range(1, 21, 2)),\n",
    "                                   'Decision_Tree__max_depth' : list(range(2, 3, 1)),\n",
    "                                   'Decision_Tree__splitter': ['best', 'random'],\n",
    "                                   'Decision_Tree__criterion' :['gini', 'entropy', 'log_loss']}, \n",
    "                                  area_under_precision_recall_curve_scorer,\n",
    "                                  'area under precision recall curve',\n",
    "                                  search_type = 'grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance improved by 0.7% and as expected PCA didn't help too much but yeah it did. The decision boundaries are slightly non-linear, already many columns went through PCA and thus, this was unhandalable by PCA. This problem should exist with ensemble models that use trees as well but maybe the voting part might help.  \n",
    "From what we have seen till now is that the decision boundary for most part is linear but on the extreme ends it tends to become non-linear and thus, a model like SVM which can handle non-linearity and models parametric equations could work or something like a deep learning model also might work. For now, I will try with XGBoost as well, if things don't workout with that also then the problem in hand might not be handeled by tree models appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first I will try decision tree without any column trasnformation for V1 to V28 columns. Since the model can adjust its complexity I  think it shouldn't make as such any difference in performance due to the transformation and I want to see that for myself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Decision tree without V1 to V28 transformations and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - Decision_Tree random search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.895\n",
      "Validation set: 0.89\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Decision_Tree__splitter': 'best',\n",
       " 'Decision_Tree__min_samples_leaf': 35,\n",
       " 'Decision_Tree__max_features': None,\n",
       " 'Decision_Tree__max_depth': 2,\n",
       " 'Decision_Tree__criterion': 'entropy'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing random search cross-validation\n",
    "\n",
    "find_best_cv_params(X_resampled,\n",
    "                    Y_resampled,\n",
    "                    Pipeline([('column_transformer', column_transformer_2), \n",
    "                              ('pca', PCA(random_state = 42)),\n",
    "                              ('standard_scaler', standard_scaler),\n",
    "                              ('Decision_Tree', DecisionTreeClassifier(class_weight = 'balanced', random_state = 123))]),                    \n",
    "                    {'Decision_Tree__max_features': [None, 'sqrt', 'log2'],\n",
    "                     'Decision_Tree__min_samples_leaf': list(range(5, 51, 5)),\n",
    "                     'Decision_Tree__max_depth' : list(range(1, 3, 1)),\n",
    "                     'Decision_Tree__splitter': ['best', 'random'],\n",
    "                     'Decision_Tree__criterion' :['gini', 'entropy', 'log_loss']}, \n",
    "                     area_under_precision_recall_curve_scorer,\n",
    "                     'area under precision recall curve',\n",
    "                     n_iter = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - Decision_Tree grid search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.895\n",
      "Validation set: 0.89\n",
      "----------------------------------------------------------\n",
      "{'Decision_Tree__criterion': 'entropy', 'Decision_Tree__max_depth': 2, 'Decision_Tree__max_features': None, 'Decision_Tree__min_samples_leaf': 29, 'Decision_Tree__splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "# Performing grid search cross-validation\n",
    "\n",
    "best_params = find_best_cv_params(X_resampled,\n",
    "                                  Y_resampled,\n",
    "                                  Pipeline([('column_transformer', column_transformer_2), \n",
    "                                          ('pca', PCA(random_state = 42)),\n",
    "                                          ('standard_scaler', standard_scaler),\n",
    "                                          ('Decision_Tree', DecisionTreeClassifier(class_weight = 'balanced', random_state = 123))]),                    \n",
    "                                  {'Decision_Tree__max_features': [None, 'sqrt', 'log2'],\n",
    "                                  'Decision_Tree__min_samples_leaf': list(range(25, 46, 2)),\n",
    "                                  'Decision_Tree__max_depth' : list(range(1, 3, 1)),\n",
    "                                  'Decision_Tree__splitter': ['best', 'random'],\n",
    "                                  'Decision_Tree__criterion' :['gini', 'entropy', 'log_loss']}, \n",
    "                                  area_under_precision_recall_curve_scorer,\n",
    "                                  'area under precision recall curve',\n",
    "                                  search_type = 'grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance increased by 0.4% compared to when the trasformations were not their in place. I guess it doesn't always work for our better to have normally distributed predictor features. Definitely need to think on it more, if anyone knows what might have happened here do let me know in the comments or through my socials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. XGBoost with class weight balancing, without V1 to V28 transformations and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - xgboost random search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.93\n",
      "Validation set: 0.925\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'xgboost__subsample': 0.9,\n",
       " 'xgboost__n_estimators': 3,\n",
       " 'xgboost__max_depth': 2,\n",
       " 'xgboost__learning_rate': np.float64(0.3),\n",
       " 'xgboost__lambda': np.float64(0.000774263682681127),\n",
       " 'xgboost__gamma': 0.001,\n",
       " 'xgboost__colsample_bytree': 0.6}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing random search cross-validation\n",
    "\n",
    "find_best_cv_params(X_resampled,\n",
    "                    Y_resampled,\n",
    "                    Pipeline([('column_transformer', column_transformer_2), \n",
    "                              ('pca', PCA(random_state = 42)),\n",
    "                              ('standard_scaler', standard_scaler),\n",
    "                              ('xgboost',XGBClassifier(objective='binary:logistic', silent=True, seed = 123, class_weight = 'balanced', error_score='raise'))]),                    \n",
    "                    {'xgboost__gamma': [0.001, 0.01, 0.5, 1, 1.5, 2, 5, 10, 15],\n",
    "                    'xgboost__learning_rate' : np.linspace(0.05, 1, 20), \n",
    "                    'xgboost__n_estimators' : list(range(2, 4)), \n",
    "                    'xgboost__subsample': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "                    'xgboost__colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "                    'xgboost__lambda': np.logspace(-4, 4, 10),\n",
    "                    'xgboost__max_depth': [1, 2, 3,]},\n",
    "                    area_under_precision_recall_curve_scorer,\n",
    "                    'area under precision recall curve',\n",
    "                    n_iter = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - xgboost grid search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.925\n",
      "Validation set: 0.925\n",
      "----------------------------------------------------------\n",
      "{'xgboost__colsample_bytree': 0.6, 'xgboost__gamma': 0.0001, 'xgboost__lambda': 0.0006, 'xgboost__learning_rate': 0.25, 'xgboost__max_depth': 2, 'xgboost__n_estimators': 3, 'xgboost__subsample': 0.85}\n"
     ]
    }
   ],
   "source": [
    "# Performing grid search cross-validation\n",
    "\n",
    "best_params = find_best_cv_params(X_resampled,\n",
    "                                  Y_resampled,\n",
    "                                  Pipeline([('column_transformer', column_transformer_2), \n",
    "                                              ('pca', PCA(random_state = 42)),\n",
    "                                              ('standard_scaler', standard_scaler),\n",
    "                                              ('xgboost',XGBClassifier(objective='binary:logistic', silent=True, seed = 123, class_weight = 'balanced', error_score='raise'))]),                    \n",
    "                                  {'xgboost__gamma': [0.0001, 0.001, 0.01],\n",
    "                                  'xgboost__learning_rate' : [0.25, 0.3, 0.35], \n",
    "                                  'xgboost__n_estimators' : list(range(2, 5)), \n",
    "                                  'xgboost__subsample': [0.85, 0.9, 0.95],\n",
    "                                  'xgboost__colsample_bytree': [0.55, 0.6, 0.65],\n",
    "                                  'xgboost__lambda': [0.0006, 0.0008, 0.001],\n",
    "                                  'xgboost__max_depth': [2, 3]},\n",
    "                                  area_under_precision_recall_curve_scorer,\n",
    "                                  'area under precision recall curve',\n",
    "                                  search_type = 'grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is just slightly better than logistic regresison model as the perforamnce improved by 0.1% comparatively but the overfitting is definitely reduced to the point that both train and validation set have same performance.  \n",
    "Last thing that I want to try will be what if we didn't drop any columns? This is ust to unerstand if the use of statistical tests has helped or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. XGBoost with class weight balancing, PCA, without V1 to V28 transformations and dropping of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - xgboost random search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.929\n",
      "Validation set: 0.92\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'xgboost__subsample': 0.7,\n",
       " 'xgboost__n_estimators': 3,\n",
       " 'xgboost__max_depth': 2,\n",
       " 'xgboost__learning_rate': np.float64(0.3),\n",
       " 'xgboost__lambda': np.float64(0.3593813663804626),\n",
       " 'xgboost__gamma': 15,\n",
       " 'xgboost__colsample_bytree': 0.9}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing random search cross-validation\n",
    "column_transformer_3 = ColumnTransformer([('double_log_transform', double_log_transformer, double_log_transform_columns)],\n",
    "                                          remainder = 'passthrough')\n",
    "\n",
    "find_best_cv_params(X_resampled,\n",
    "                    Y_resampled,\n",
    "                    Pipeline([('column_transformer', column_transformer_3), \n",
    "                              ('pca', PCA(random_state = 42)),\n",
    "                              ('standard_scaler', standard_scaler),\n",
    "                              ('xgboost',XGBClassifier(objective='binary:logistic', silent=True, seed = 123, class_weight = 'balanced', error_score='raise'))]),                    \n",
    "                    {'xgboost__gamma': [0.001, 0.01, 0.5, 1, 1.5, 2, 5, 10, 15],\n",
    "                    'xgboost__learning_rate' : np.linspace(0.05, 1, 20), \n",
    "                    'xgboost__n_estimators' : list(range(2, 4)), \n",
    "                    'xgboost__subsample': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "                    'xgboost__colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "                    'xgboost__lambda': np.logspace(-4, 4, 10),\n",
    "                    'xgboost__max_depth': [1, 2, 3,]},\n",
    "                    area_under_precision_recall_curve_scorer,\n",
    "                    'area under precision recall curve',\n",
    "                    n_iter = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - xgboost grid search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.926\n",
      "Validation set: 0.924\n",
      "----------------------------------------------------------\n",
      "{'xgboost__colsample_bytree': 0.9, 'xgboost__gamma': 20, 'xgboost__lambda': 0.26, 'xgboost__learning_rate': 0.35, 'xgboost__max_depth': 3, 'xgboost__n_estimators': 3, 'xgboost__subsample': 0.65}\n"
     ]
    }
   ],
   "source": [
    "# Performing grid search cross-validation\n",
    "\n",
    "best_params = find_best_cv_params(X_resampled,\n",
    "                                  Y_resampled,\n",
    "                                  Pipeline([('column_transformer', column_transformer_3), \n",
    "                                              ('pca', PCA(random_state = 42)),\n",
    "                                              ('standard_scaler', standard_scaler),\n",
    "                                              ('xgboost',XGBClassifier(objective='binary:logistic', silent=True, seed = 123, class_weight = 'balanced', error_score='raise'))]),                    \n",
    "                                  {'xgboost__gamma': [10, 15, 20],\n",
    "                                  'xgboost__learning_rate' : [0.25, 0.3, 0.35], \n",
    "                                  'xgboost__n_estimators' : list(range(2, 5)), \n",
    "                                  'xgboost__subsample': [0.65, 0.7, 0.75],\n",
    "                                  'xgboost__colsample_bytree': [0.85, 0.9, 0.95],\n",
    "                                  'xgboost__lambda': [0.26, 0.36, 0.46],\n",
    "                                  'xgboost__max_depth': [2, 3]},\n",
    "                                  area_under_precision_recall_curve_scorer,\n",
    "                                  'area under precision recall curve',\n",
    "                                  search_type = 'grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance has slightly decreased and this is a good indication that our statistical tests worked fine and including them has decreased the performance slightly as the algorithm might be capturing the noise within them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conclusion of experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In conlusion of the experimentation:**  \n",
    "- Normalizaing transformations or skewness reducing transformations for V1 to V28 columns are not effective here at least. But the transformations for amount column is slightly benefecial.  \n",
    "- Our feature selection stood up to its reputation and thus, proved to be benfecial.  \n",
    "- Simpler models like logistic regression are performing almost same as much more complex models like xgboost while decision tree performed relatively quite poorly.  \n",
    "- PCA helped improve performance of tree models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Going on ahead:**  \n",
    "- Will use the conclusions from above to start training our final models which can then ultimately be evaluate don our unseen test set.  \n",
    "- I will try logistic regression, XGBoost, Support vector machine and deep learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - Logistic_Regression random search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.936\n",
      "Validation set: 0.924\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Logistic_Regression__solver': 'lbfgs',\n",
       " 'Logistic_Regression__penalty': None,\n",
       " 'Logistic_Regression__l1_ratio': np.float64(0.6),\n",
       " 'Logistic_Regression__C': np.float64(206.913808111479)}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_transformer_final = ColumnTransformer([('double_log_transform', double_log_transformer, double_log_transform_columns),\n",
    "                                              ('drop', 'drop', drop_columns)],\n",
    "                                              remainder = 'passthrough')\n",
    "\n",
    "# Performing random search cross-validation\n",
    "\n",
    "find_best_cv_params(X_resampled,\n",
    "                    Y_resampled,\n",
    "                    Pipeline([('column_transformer', column_transformer_final), \n",
    "                              ('standard_scaler', standard_scaler),\n",
    "                              ('Logistic_Regression', LogisticRegression(class_weight = 'balanced', n_jobs = -1, random_state = 123))]),\n",
    "                    {'Logistic_Regression__penalty' : ['l1', 'l2', 'elasticnet', None],\n",
    "                     'Logistic_Regression__C' : np.logspace(-4, 4, 20), \n",
    "                     'Logistic_Regression__solver': ['lbfgs', 'newton-cg', 'sag', 'saga'],\n",
    "                     'Logistic_Regression__l1_ratio': np.linspace(0.1, 1, 9, endpoint = False)}, \n",
    "                    area_under_precision_recall_curve_scorer,\n",
    "                    'area under precision recall curve',\n",
    "                    n_iter = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While doing final trainings, I will try to store the model and best grid search parameters so that I can use them later on for final training on the full train set and also, for my end-to-end deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.append({'name': 'Logistic_Regression',\n",
    "               'pipeline': Pipeline([('column_transformer', column_transformer_final), \n",
    "                                     ('standard_scaler', standard_scaler),\n",
    "                                     ('Logistic_Regression', LogisticRegression(class_weight = 'balanced', n_jobs = -1, random_state = 123))])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - Logistic_Regression grid search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.936\n",
      "Validation set: 0.924\n",
      "----------------------------------------------------------\n",
      "{'Logistic_Regression__penalty': None, 'Logistic_Regression__solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "# perform grid search cross-validation\n",
    "\n",
    "models[0]['best_params'] = find_best_cv_params(X_resampled,\n",
    "                                               Y_resampled,\n",
    "                                               models[0]['pipeline'],\n",
    "                                               {'Logistic_Regression__penalty' : [None],\n",
    "                                                'Logistic_Regression__solver': ['lbfgs']}, \n",
    "                                               area_under_precision_recall_curve_scorer,\n",
    "                                               'area under precision recall curve',\n",
    "                                               search_type = 'grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - Support_Vector_Classifier random search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.905\n",
      "Validation set: 0.904\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Support_Vector_Classifier__kernel': 'poly',\n",
       " 'Support_Vector_Classifier__degree': 1,\n",
       " 'Support_Vector_Classifier__C': np.float64(0.0001062467830894041)}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing random search cross-validation\n",
    "\n",
    "find_best_cv_params(X_resampled,\n",
    "                    Y_resampled,\n",
    "                    Pipeline([('column_transformer', column_transformer_final), \n",
    "                                     ('standard_scaler', standard_scaler),\n",
    "                                     ('Support_Vector_Classifier', SVC(probability = True, class_weight = 'balanced', random_state = 42))]),\n",
    "                    {'Support_Vector_Classifier__C' : np.logspace(-5, -3.5, 20), \n",
    "                     'Support_Vector_Classifier__kernel': ['poly'],\n",
    "                     'Support_Vector_Classifier__degree': [1, 2]}, \n",
    "                    area_under_precision_recall_curve_scorer,\n",
    "                    'area under precision recall curve',\n",
    "                    n_iter = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.append({'name': 'Suppot_Vector_Classifier',\n",
    "               'pipeline': Pipeline([('column_transformer', column_transformer_final), \n",
    "                                     ('standard_scaler', standard_scaler),\n",
    "                                     ('Support_Vector_Classifier', SVC(probability = True, class_weight = 'balanced', random_state = 42))])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - Support_Vector_Classifier grid search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.914\n",
      "Validation set: 0.912\n",
      "----------------------------------------------------------\n",
      "{'Support_Vector_Classifier__C': np.float64(0.00031622776601683794), 'Support_Vector_Classifier__degree': 1, 'Support_Vector_Classifier__kernel': 'poly'}\n"
     ]
    }
   ],
   "source": [
    "# perform grid search cross-validation\n",
    "\n",
    "models[1]['best_params'] = find_best_cv_params(X_resampled,\n",
    "                                               Y_resampled,\n",
    "                                               models[1]['pipeline'],\n",
    "                                               {'Support_Vector_Classifier__C' : np.logspace(-5, -3.5, 10), \n",
    "                                                'Support_Vector_Classifier__kernel': ['poly'],\n",
    "                                                'Support_Vector_Classifier__degree': [1]}, \n",
    "                                               area_under_precision_recall_curve_scorer,\n",
    "                                               'area under precision recall curve',\n",
    "                                               search_type = 'grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - XGBoost random search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.93\n",
      "Validation set: 0.925\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'XGBoost__subsample': 0.9,\n",
       " 'XGBoost__n_estimators': 3,\n",
       " 'XGBoost__max_depth': 2,\n",
       " 'XGBoost__learning_rate': np.float64(0.3),\n",
       " 'XGBoost__lambda': np.float64(0.000774263682681127),\n",
       " 'XGBoost__gamma': 0.001,\n",
       " 'XGBoost__colsample_bytree': 0.6}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing random search cross-validation\n",
    "\n",
    "find_best_cv_params(X_resampled,\n",
    "                    Y_resampled,\n",
    "                    Pipeline([('column_transformer', column_transformer_final), \n",
    "                              ('pca', PCA(random_state = 42)),\n",
    "                              ('standard_scaler', standard_scaler),\n",
    "                              ('XGBoost',XGBClassifier(objective='binary:logistic', silent=True, seed = 123, class_weight = 'balanced', error_score='raise'))]),                    \n",
    "                    {'XGBoost__gamma': [0.001, 0.01, 0.5, 1, 1.5, 2, 5, 10, 15],\n",
    "                     'XGBoost__learning_rate' : np.linspace(0.05, 1, 20), \n",
    "                     'XGBoost__n_estimators' : list(range(2, 4)), \n",
    "                     'XGBoost__subsample': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "                     'XGBoost__colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "                     'XGBoost__lambda': np.logspace(-4, 4, 10),\n",
    "                     'XGBoost__max_depth': [1, 2, 3,]},\n",
    "                    area_under_precision_recall_curve_scorer,\n",
    "                    'area under precision recall curve',\n",
    "                    n_iter = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.append({'name': 'XGBoost',\n",
    "               'pipeline': Pipeline([('column_transformer', column_transformer_final), \n",
    "                                     ('pca', PCA(random_state = 42)),\n",
    "                                     ('standard_scaler', standard_scaler),\n",
    "                                     ('XGBoost',XGBClassifier(objective='binary:logistic', silent=True, seed = 123, class_weight = 'balanced', error_score='raise'))])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - XGBoost grid search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.925\n",
      "Validation set: 0.925\n",
      "----------------------------------------------------------\n",
      "{'XGBoost__colsample_bytree': 0.6, 'XGBoost__gamma': 0.0001, 'XGBoost__lambda': 0.00057, 'XGBoost__learning_rate': 0.25, 'XGBoost__max_depth': 2, 'XGBoost__n_estimators': 3, 'XGBoost__subsample': 0.85}\n"
     ]
    }
   ],
   "source": [
    "# perform grid search cross-validation\n",
    "\n",
    "models[2]['best_params'] = find_best_cv_params(X_resampled,\n",
    "                                               Y_resampled,\n",
    "                                               models[2]['pipeline'],\n",
    "                                               {'XGBoost__gamma': [0.0001, 0.001, 0.01],\n",
    "                                                'XGBoost__learning_rate' : [0.25, 0.3, 0.35], \n",
    "                                                'XGBoost__n_estimators' : list(range(2, 5)), \n",
    "                                                'XGBoost__subsample': [0.85, 0.9, 0.95],\n",
    "                                                'XGBoost__colsample_bytree': [0.55, 0.6, 0.65],\n",
    "                                                'XGBoost__lambda': [0.00057, 0.000774263682681127, 0.00097],\n",
    "                                                'XGBoost__max_depth': [2, 3]},\n",
    "                                               area_under_precision_recall_curve_scorer,\n",
    "                                               'area under precision recall curve',\n",
    "                                               search_type = 'grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Deep Learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - DL_Model random search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.92\n",
      "Validation set: 0.912\n",
      "----------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'DL_Model__learning_rate_init': np.float64(0.007847599703514606),\n",
       " 'DL_Model__hidden_layer_sizes': (1,)}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing random search cross-validation\n",
    "\n",
    "clf = MLPClassifier(activation = 'relu',\n",
    "                    solver = 'adam',\n",
    "                    batch_size = 32,\n",
    "                    learning_rate = 'constant',\n",
    "                    max_iter = 200,\n",
    "                    shuffle = True,\n",
    "                    random_state = 42,\n",
    "                    early_stopping = True,\n",
    "                    n_iter_no_change = 20,\n",
    "                    verbose = True)\n",
    "\n",
    "dl_pipeline = Pipeline([('column_transformer', column_transformer_final),\n",
    "                        ('standard_scaler', standard_scaler),\n",
    "                        ('DL_Model', clf)])\n",
    "\n",
    "find_best_cv_params(X_resampled,\n",
    "                    Y_resampled,\n",
    "                    dl_pipeline,                    \n",
    "                    {'DL_Model__hidden_layer_sizes': [(1,), (1, 1)],\n",
    "                     'DL_Model__learning_rate_init' : np.logspace(-4, -1, 20)},\n",
    "                    area_under_precision_recall_curve_scorer,\n",
    "                    'area under precision recall curve',\n",
    "                    n_iter = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.append({'name': 'DL_Model',\n",
    "               'pipeline': dl_pipeline})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For model - DL_Model grid search:\n",
      "----------------------------------------------------------\n",
      "Best mean area under precision recall curve:\n",
      "Train set: 0.921\n",
      "Validation set: 0.914\n",
      "----------------------------------------------------------\n",
      "{'DL_Model__hidden_layer_sizes': (1, 1), 'DL_Model__learning_rate_init': np.float64(0.009236708571873866)}\n"
     ]
    }
   ],
   "source": [
    "# perform grid search cross-validation\n",
    "\n",
    "models[3]['best_params'] = find_best_cv_params(X_resampled,\n",
    "                                               Y_resampled,\n",
    "                                               models[3]['pipeline'],\n",
    "                                               {'DL_Model__hidden_layer_sizes': [(1,), (1, 1)],\n",
    "                                                'DL_Model__learning_rate_init' : np.logspace(-4, -1, 30)},\n",
    "                                               area_under_precision_recall_curve_scorer,\n",
    "                                               'area under precision recall curve',\n",
    "                                               search_type = 'grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the layers that the model needed is not too much but still the model was not able to reach the performance levels of logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://www.kdnuggets.com/2023/01/7-smote-variations-oversampling.html\n",
    "- https://www.analyticsvidhya.com/blog/2020/10/overcoming-class-imbalance-using-smote-techniques/#h-dealing-with-imbalanced-data\n",
    "- https://scikit-learn.org/stable/\n",
    "- https://www.youtube.com/@krishnaik06  \n",
    "- Hands on Machine Learning with Scikit-Learn & TensorFlow by Aurlien Gron (O'Reilly). CopyRight 2017 Aurlien Gron"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
